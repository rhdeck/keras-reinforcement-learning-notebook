{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Minor extension of gist at https://gist.github.com/EderSantana/c7222daa328f0e885093\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "from IPython import display\n",
    "from tqdm import trange\n",
    "class Catch(object):\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        if action == 0:  # left\n",
    "            action = -1\n",
    "        elif action == 1:  # stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1  # right\n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "\n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
    "        return canvas\n",
    "        \n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):\n",
    "        if self.state[0, 0] == self.grid_size-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0, n, m])[np.newaxis]\n",
    "\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i:i+1] = state_t\n",
    "            # There should be no target values for actions not taken.\n",
    "            # Thou shalt not correct actions not taken #deep\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # reward_t + gamma * max_a' Q(s', a')\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "\n",
    "   \n",
    "# parameters\n",
    "epsilon = .1  # exploration\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "epoch = 1000\n",
    "max_memory = 500\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "grid_size = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "model.add(Dense(hidden_size, activation='relu'))\n",
    "model.add(Dense(num_actions))\n",
    "model.compile(sgd(lr=.2), \"mse\")\n",
    "    \n",
    "# If you want to continue training from a previous model, just uncomment the line bellow\n",
    "# model.load_weights(\"model.h5\")\n",
    "\n",
    "# Define environment/game\n",
    "env = Catch(grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
    "\n",
    "def train(model ,epochs):\n",
    "    # Train\n",
    "    win_cnt = 0\n",
    "    for e in trange(epochs):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "            # get next action\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, num_actions, size=1)[0]\n",
    "            else:\n",
    "                q = model.predict(input_tm1)\n",
    "                action = np.argmax(q[0])\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            if reward == 1:\n",
    "                win_cnt += 1\n",
    "\n",
    "            # store experience\n",
    "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)            \n",
    "            \n",
    "            # adapt model\n",
    "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "            \n",
    "            if e%50==0:\n",
    "                #display_screen(action,win_cnt,inputs[0])  \n",
    "                display.clear_output(wait=True)\n",
    "                print(\"Epoch {}/{} | Loss {:.4f} | Win count {}\".format(e, epochs, loss, win_cnt, ))\n",
    "                \n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "        display.clear_output(wait=True)\n",
    "        print(\"Epoch {}/{} | Loss {:.4f} | Win count {}\".format(e, epochs, loss, win_cnt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.models import model_from_json\n",
    "#from qlearn import Catch\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "last_frame_time = 0\n",
    "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
    "grid_size = 10\n",
    "\n",
    "\n",
    "def set_max_fps(last_frame_time,FPS = 1):\n",
    "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    return current_milli_time()\n",
    "def display_screen(action,points,input_t, games):\n",
    "    global last_frame_time\n",
    "    display.clear_output(wait=True)\n",
    "    if(\"End\" not in translate_action[action]):\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "               interpolation='none', cmap='gray')\n",
    "        display.display(plt.gcf())\n",
    "    last_frame_time = set_max_fps(last_frame_time)\n",
    "    print(\"Action %s, Wins/Games: %d/%d\" % (translate_action[action],points,games))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999/5000 | Loss 0.0130 | Win count 3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5000/5000 [09:12<00:00,  9.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#This one is fun to run repeatedly and watch the win count improve over repeated training sessions.\n",
    "train(model, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a test for running the game repeatedly turn by turn and saving whether it succeeded or failed. \n",
    "def test(model, games):\n",
    "    global last_frame_time\n",
    "    plt.ion()\n",
    "    # Define environment, game\n",
    "    env = Catch(grid_size)\n",
    "    c = 0\n",
    "    last_frame_time = 0\n",
    "    points = 0\n",
    "    for e in range(games):\n",
    "        loss = 0.\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "        display_screen(3,points,input_t,e)\n",
    "        c += 1\n",
    "        while not game_over:\n",
    "            input_tm1 = input_t\n",
    "            # get next action\n",
    "            q = model.predict(input_tm1)\n",
    "            action = np.argmax(q[0])\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            if(reward == 1): \n",
    "                points += reward\n",
    "            display_screen(action,points, input_t, e)\n",
    "            c += 1\n",
    "    print(\"Points: %d\" % (points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24bceace358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Left, Wins/Games: 20/19\n",
      "Points: 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24bceace358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Runs tests and shows them inline. \n",
    "test(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
